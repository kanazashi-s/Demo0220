
# coding: utf-8

# - ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã‚„ã‚¯ã‚»ã®å¼·ã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¤šç”¨ã•ã‚Œã¦ãŠã‚Šã€èª­ã¿ã¥ã‚‰ããªã£ã¦ã„ã¾ã™ã€‚
# - [éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°èªè­˜](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/speech/cloud-client)(transcribe.pyãªã©)ã‚„ã€[æ„Ÿæƒ…æ¨å®šã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/language/sentiment)ã«ç›®ã‚’é€šã—ã¦ã‹ã‚‰èª­ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

# In[ ]:





# In[ ]:


from __future__ import division

import re
import json
import sys
import argparse

from google.cloud import speech
from google.cloud.speech import enums
from google.cloud.speech import types
import googleapiclient.discovery

import pyaudio
from six.moves import queue


# In[ ]:


# Audio recording parameters
RATE = 16000
CHUNK = int(RATE / 10)  # 100ms

import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"]= "C:/Users/USER02/Documents/Voice2Text.json"


# ### ğŸæ„Ÿæƒ…æ¨å®šç”¨ã®é–¢æ•°ã‚’å®šç¾©
# - get_native_encoding_type()ã¯ã€åˆ©ç”¨ã—ã¦ã„ã‚‹pythonã®æ–‡å­—åˆ—ã®ã‚¿ã‚¤ãƒ—ãŒutf-16ã‹utf-32ã‹ã‚’è¿”ã™
# - analyze_sentimentã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™ã¨æ„Ÿæƒ…èªè­˜ã®çµæœï¼ˆè¾æ›¸å‹ï¼‰ã‚’è¿”ã™

# In[ ]:


def get_native_encoding_type():
    """Returns the encoding type that matches Python's native strings."""
    if sys.maxunicode == 65535:
        return 'UTF16'
    else:
        return 'UTF32'
    


def analyze_sentiment(text, encoding='utf-8'):
    body = {
        'document': {
            'type': 'PLAIN_TEXT',
            'content': text,
        },
        'encoding_type': encoding
    }

    service = googleapiclient.discovery.build('language', 'v1')

    request = service.documents().analyzeSentiment(body=body)
    response = request.execute()

    return response


# ### ğŸPyaudioã‚’ç”¨ã„ã¦ãƒã‚¤ã‚¯ã‹ã‚‰éŸ³å£°ã‚’å–å¾—ã™ã‚‹

# In[ ]:


class MicrophoneStream(object):
    """
    Opens a recording stream as a generator yielding the audio chunks.
    éŸ³å£°ã®ãƒãƒ£ãƒ³ã‚¯ã‚’yieldã™ã‚‹éŒ²éŸ³ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã™ã‚‹ï¼ˆï¼Ÿï¼Ÿï¼Ÿï¼‰
    """
    def __init__(self, rate, chunk):
        self._rate = rate
        self._chunk = chunk

        # Create a thread-safe buffer of audio data
        # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã®ã€Œã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒãƒƒãƒ•ã‚¡ã€ï¼Ÿï¼Ÿï¼Ÿã‚’ä½œæˆã™ã‚‹ã€‚
        self._buff = queue.Queue()
        self.closed = True

    def __enter__(self):
        self._audio_interface = pyaudio.PyAudio()
        self._audio_stream = self._audio_interface.open(
            format=pyaudio.paInt16,
            # The API currently only supports 1-channel (mono) audio
            
            # ã“ã®APIã¯ã€ç¾åœ¨ã®ã¨ã“ã‚ãƒ¢ãƒãƒ©ãƒ«éŸ³å£°ã«ã—ã‹å¯¾å¿œã—ã¦ãªã„ã€‚
            # https://goo.gl/z757pE
            channels=1, rate=self._rate,
            input=True, frames_per_buffer=self._chunk,
            # Run the audio stream asynchronously to fill the buffer object.
            # This is necessary so that the input device's buffer doesn't
            # overflow while the calling thread makes network requests, etc.
            
            # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ã€ãƒãƒƒãƒ•ã‚¡ã‚’åŸ‹ã‚ã‚‹ãŸã‚ã«éåŒæœŸã§å‹•ã‹ãã†ã€‚
            # ã“ã‚Œã¯ã€å‘¼ã³å‡ºã—ã¦ã„ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ãŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã—ã¦ã„ã‚‹é–“ãªã©ã§ã‚ã£ã¦ã‚‚
            # å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ã®ãƒãƒƒãƒ•ã‚¡ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ã‚¦ã‚’èµ·ã“ã•ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã€å¿…è¦ã§ã‚ã‚‹ã€‚
            stream_callback=self._fill_buffer,
        )

        self.closed = False

        return self

    def __exit__(self, type, value, traceback):
        self._audio_stream.stop_stream()
        self._audio_stream.close()
        self.closed = True
        # Signal the generator to terminate so that the client's
        # streaming_recognize method will not block the process termination.
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã«çµ‚äº†ã®ä¿¡å·ã‚’ä¼ãˆã‚‹ã€‚
        # ã˜ã‚ƒãªã„ã¨ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒã€ãƒ—ãƒ­ã‚»ã‚¹ã®çµ‚äº†ã‚’é˜»å®³ã—ã¦ã—ã¾ã†ã€‚
        self._buff.put(None)
        self._audio_interface.terminate()

    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):
        """Continuously collect data from the audio stream, into the buffer."""
        self._buff.put(in_data)
        return None, pyaudio.paContinue

    def generator(self):
        while not self.closed:
            # Use a blocking get() to ensure there's at least one chunk of
            # data, and stop iteration if the chunk is None, indicating the
            # end of the audio stream.
            # ï¼ˆæ—¥è¨³ï¼‰get()ã‚’ç”¨ã„ã¦ã€å°‘ãªãã¨ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒ1ãƒãƒ£ãƒ³ã‚¯ä»¥ä¸Šã‚ã‚‹ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
            # ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ£ãƒ³ã‚¯æ•°ãŒNoneã‚’ç¤ºã™ã¨ãã€éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¯çµ‚äº†ã—ã¦ã„ã¾ã™ã€‚
            chunk = self._buff.get()
            if chunk is None:
                return
            data = [chunk]

            # Now consume whatever other data's still buffered.
            # ï¼ˆæ—¥è¨³ï¼‰ï¼Ÿï¼Ÿï¼Ÿ
            while True:
                try:
                    chunk = self._buff.get(block=False)
                    if chunk is None:
                        return
                    data.append(chunk)
                except queue.Empty:
                    break
            # b''ã¯ã€ãƒã‚¤ãƒˆå‹ã§æ–‡å­—åˆ—ã‚’é€ä¿¡ã™ã‚‹ã¨ã„ã†è¨˜å·
            yield b''.join(data)


# ### ğŸãƒã‚¤ã‚¯ã‹ã‚‰éŸ³å£°ã‚’å–å¾—ã—ã€ã‚µãƒ¼ãƒã«é€ä¿¡ã™ã‚‹å‡¦ç†ã‚’ç¹°ã‚Šè¿”ã™é–¢æ•°

# In[ ]:


def listen_print_loop(responses, sentiment):
    
    """
    Iterates through server responses and prints them.
    # ã‚µãƒ¼ãƒã®å¿œç­”ã‚’ç¹°ã‚Šè¿”ã—ã¦ã€ãã‚Œã‚‰ã‚’å°åˆ·ã—ã¾ã™ã€‚

    The responses passed is a generator that will block until a response
    is provided by the server.
    æ¸¡ã•ã‚Œã‚‹å¿œç­”ã¯ã€å¿œç­”ãŒã‚ã‚‹ã¾ã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã§ã™ã€‚
    ã‚µãƒ¼ãƒãƒ¼ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã¾ã™ã€‚

    """
    
    num_chars_printed = 0
    
    for response in responses:
        if not response.results:
            continue

        # The `results` list is consecutive. For streaming, we only care about
        # the first result being considered, since once it's `is_final`, it
        # moves on to considering the next utterance.
        
        # 'sesults'ã®ãƒªã‚¹ãƒˆã¯ã€é€£ç¶šã—ã¦ã„ã‚‹(ãŠãã‚‰ãã€ãŸãã•ã‚“ã®è¦ç´ ãŒå«ã¾ã‚Œã‚‹ã¨ã„ã†ã“ã¨)
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èªè­˜ã®å ´åˆã¯ã€æœ€åˆã®çµæœã®ã¿è€ƒæ…®ã™ã‚‹ã€‚
        # ãªãœãªã‚‰ã€ã„ã£ãŸã‚“'is_final'ã«ãªã£ãŸã‚‰ã€æ¬¡ã®ç™ºè©±ã®å‡¦ç†ã«ç§»ã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚
        result = response.results[0]
        if not result.alternatives:
            continue

        # Display the transcription of the top alternative.
        # ãƒˆãƒƒãƒ—ã®ä»£æ›¿æ¡ˆã‚’è¡¨ç¤ºã™ã‚‹
        transcript = result.alternatives[0].transcript

        # Display interim results, but with a carriage return at the end of the
        # line, so subsequent lines will overwrite them.
        
        # æš«å®šã®çµæœã‚’è¡¨ç¤ºã™ã‚‹ã€‚ãŸã ã—ã€è¡Œæœ«ã«ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ï¼Ÿï¼Ÿï¼ŸãŒã‚ã‚‹ã®ã§
        # å¾Œç¶šã®è¡Œã¯ã“ã‚Œã‚‰ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚
        #
        # If the previous result was longer than this one, we need to print
        # some extra spaces to overwrite the previous result
        
        # ã‚‚ã—ä¸€å€‹å‰ã®çµæœãŒä»Šå›ã®ã‚‚ã®ã‚ˆã‚Šã‚‚é•·ã‹ã£ãŸã‚‰ã€æˆ‘ã€…ã¯è¿½åŠ ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’
        # å‰å›ã®çµæœã‚’ä¸Šæ›¸ãã™ã‚‹ãŸã‚ã«è¿½åŠ ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚
        overwrite_chars = ' ' * (num_chars_printed - len(transcript))

        if not result.is_final:
            sys.stdout.write(transcript + overwrite_chars + '\r')
            sys.stdout.flush()

            num_chars_printed = len(transcript)

        else:
            print_str = transcript + overwrite_chars
            
            if sentiment:
                
                mag, score = analyze_sentiment(transcript + overwrite_chars, get_native_encoding_type())['documentSentiment'].values()
                print_str += '\næ„Ÿæƒ…ã®æ­£è² :{} æ„Ÿæƒ…ã®å¼·ã•:{}\n'.format(score, mag)
                
            yield print_str
            

            # Exit recognition if any of the transcribed phrases could be
            # one of our keywords.
            
            # è»¢è¨˜ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ã‚ºã®ã„ãšã‚Œã‹ãŒGoogleã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®1ã¤ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã¯
            # èªè­˜ã‚’çµ‚äº†ã—ã¦ãã ã•ã„ã€‚
            if re.search(r'\b(exit|quit)\b', transcript, re.I):
                print('Exiting..')
                break

            num_chars_printed = 0


# ### ğŸãƒ¡ã‚¤ãƒ³é–¢æ•°
# - è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆ[è©³ç´°](https://cloud.google.com/speech-to-text/docs/languages)ï¼‰ã¨ã€æ„Ÿæƒ…æ¨å®šã™ã‚‹ã‹å¦ã‚’å¼•æ•°ã«ã¨ã‚‹
# - ã“ã‚Œå‘¼ã³å‡ºã™ã¹ã—

# In[ ]:


def mic_recog(lang='ja-JP', sentiment=False):
    # See http://g.co/cloud/speech/docs/languages
    # for a list of supported languages.
    language_code = lang  # a BCP-47 language tag

    client = speech.SpeechClient()
    config = types.RecognitionConfig(
        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=RATE,
        language_code=language_code)
    streaming_config = types.StreamingRecognitionConfig(
        config=config,
        interim_results=True)

    with MicrophoneStream(RATE, CHUNK) as stream:
        audio_generator = stream.generator()
        requests = (types.StreamingRecognizeRequest(audio_content=content)
                    for content in audio_generator)

        responses = client.streaming_recognize(streaming_config, requests)
        
        loopitr = listen_print_loop(responses, sentiment)
        
        try:
            while True:
                # Now, put the transcription responses to use.
                print(next(loopitr))
        except KeyboardInterrupt:
            print('\nğŸŒ¸ğŸŒ¸ğŸŒ¸Interrupted!!!ğŸŒ¸ğŸŒ¸ğŸŒ¸')
            return 0
        except :
            return -1


# ### ğŸå®Ÿè¡Œéƒ¨åˆ†
# - mainï¼ˆè¨€èªã‚³ãƒ¼ãƒ‰, æ„Ÿæƒ…æ¨å®šOnOffï¼‰
# - ã‚‚ã—65ç§’ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ä¸­æ–­ã—ã¦ã—ã¾ã£ãŸå ´åˆã¯ã€ã‚‚ã†ä¸€åº¦å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«ç„¡é™ãƒ«ãƒ¼ãƒ—

# In[ ]:


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument(
        '-lang', help='Language code ex.en-US, zh, etc...(default:ja-JP)', default='ja-JP')
    parser.add_argument(
        '-sentiment', help='If you want to analyze sentiment, enter "True"', default=False)
    args = parser.parse_args()

    while True:
        result = mic_recog(args.lang, args.sentiment)
        if result == 0:
            break


