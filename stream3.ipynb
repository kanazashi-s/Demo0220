{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- このコードは、ストリーミングで動作することを目的としているため、ジェネレータやクセの強いライブラリが多用されており、読みづらくなっています。\n",
    "- 非ストリーミングのソースコードを理解してから読むことをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= \"C:/Users/USER02/Documents/Voice2Text.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "import googleapiclient.discovery\n",
    "\n",
    "import pyaudio\n",
    "from six.moves import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio recording parameters\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE / 10)  # 100ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐍感情推定用の関数を定義\n",
    "- get_native_encoding_type()は、利用しているpythonの文字列のタイプがutf-16かutf-32かを返す\n",
    "- analyze_sentimentは、テキストを引数として渡すと感情認識の結果（辞書型）を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_native_encoding_type():\n",
    "    \"\"\"Returns the encoding type that matches Python's native strings.\"\"\"\n",
    "    if sys.maxunicode == 65535:\n",
    "        return 'UTF16'\n",
    "    else:\n",
    "        return 'UTF32'\n",
    "    \n",
    "\n",
    "\n",
    "def analyze_sentiment(text, encoding='utf-8'):\n",
    "    body = {\n",
    "        'document': {\n",
    "            'type': 'PLAIN_TEXT',\n",
    "            'content': text,\n",
    "        },\n",
    "        'encoding_type': encoding\n",
    "    }\n",
    "\n",
    "    service = googleapiclient.discovery.build('language', 'v1')\n",
    "\n",
    "    request = service.documents().analyzeSentiment(body=body)\n",
    "    response = request.execute()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐍Pyaudioを用いてマイクから音声を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicrophoneStream(object):\n",
    "    \"\"\"\n",
    "    Opens a recording stream as a generator yielding the audio chunks.\n",
    "    音声のチャンクをyieldする録音のストリームをオープンする（？？？）\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, chunk):\n",
    "        self._rate = rate\n",
    "        self._chunk = chunk\n",
    "\n",
    "        # Create a thread-safe buffer of audio data\n",
    "        # オーディオデータの「スレッドセーフバッファ」？？？を作成する。\n",
    "        self._buff = queue.Queue()\n",
    "        self.closed = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._audio_interface = pyaudio.PyAudio()\n",
    "        self._audio_stream = self._audio_interface.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            # The API currently only supports 1-channel (mono) audio\n",
    "            \n",
    "            # このAPIは、現在のところモノラル音声にしか対応してない。\n",
    "            # https://goo.gl/z757pE\n",
    "            channels=1, rate=self._rate,\n",
    "            input=True, frames_per_buffer=self._chunk,\n",
    "            # Run the audio stream asynchronously to fill the buffer object.\n",
    "            # This is necessary so that the input device's buffer doesn't\n",
    "            # overflow while the calling thread makes network requests, etc.\n",
    "            \n",
    "            # オーディオストリームを、バッファを埋めるために非同期で動かそう。\n",
    "            # これは、呼び出しているスレッドがネットワークにリクエストをしている間などであっても\n",
    "            # 入力デバイスのバッファがオーバーフロウを起こさないようにするため、必要である。\n",
    "            stream_callback=self._fill_buffer,\n",
    "        )\n",
    "\n",
    "        self.closed = False\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self._audio_stream.stop_stream()\n",
    "        self._audio_stream.close()\n",
    "        self.closed = True\n",
    "        # Signal the generator to terminate so that the client's\n",
    "        # streaming_recognize method will not block the process termination.\n",
    "        # ジェネレータに終了の信号を伝える。\n",
    "        # じゃないと、ストリーミング認識のメソッドが、プロセスの終了を阻害してしまう。\n",
    "        self._buff.put(None)\n",
    "        self._audio_interface.terminate()\n",
    "\n",
    "    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):\n",
    "        \"\"\"Continuously collect data from the audio stream, into the buffer.\"\"\"\n",
    "        self._buff.put(in_data)\n",
    "        return None, pyaudio.paContinue\n",
    "\n",
    "    def generator(self):\n",
    "        while not self.closed:\n",
    "            # Use a blocking get() to ensure there's at least one chunk of\n",
    "            # data, and stop iteration if the chunk is None, indicating the\n",
    "            # end of the audio stream.\n",
    "            # （日訳）get()を用いて、少なくともデータが1チャンク以上あるかを確認してください。\n",
    "            # データのチャンク数がNoneを示すとき、音声ストリームは終了しています。\n",
    "            chunk = self._buff.get()\n",
    "            if chunk is None:\n",
    "                return\n",
    "            data = [chunk]\n",
    "\n",
    "            # Now consume whatever other data's still buffered.\n",
    "            # （日訳）？？？\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = self._buff.get(block=False)\n",
    "                    if chunk is None:\n",
    "                        return\n",
    "                    data.append(chunk)\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "            # b''は、バイト型で文字列を送信するという記号\n",
    "            yield b''.join(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐍マイクから音声を取得し、サーバに送信する処理を繰り返す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen_print_loop(responses, sentiment):\n",
    "    \n",
    "    \"\"\"\n",
    "    Iterates through server responses and prints them.\n",
    "    # サーバの応答を繰り返して、それらを印刷します。\n",
    "\n",
    "    The responses passed is a generator that will block until a response\n",
    "    is provided by the server.\n",
    "    渡される応答は、応答があるまでブロックされるジェネレータです。\n",
    "    サーバーによって提供されます。\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    num_chars_printed = 0\n",
    "    \n",
    "    for response in responses:\n",
    "        if not response.results:\n",
    "            continue\n",
    "\n",
    "        # The `results` list is consecutive. For streaming, we only care about\n",
    "        # the first result being considered, since once it's `is_final`, it\n",
    "        # moves on to considering the next utterance.\n",
    "        \n",
    "        # 'sesults'のリストは、連続している(おそらく、たくさんの要素が含まれるということ)\n",
    "        # ストリーミング認識の場合は、最初の結果のみ考慮する。\n",
    "        # なぜなら、いったん'is_final'になったら、次の発話の処理に移るからである。\n",
    "        result = response.results[0]\n",
    "        if not result.alternatives:\n",
    "            continue\n",
    "\n",
    "        # Display the transcription of the top alternative.\n",
    "        # トップの代替案を表示する\n",
    "        transcript = result.alternatives[0].transcript\n",
    "\n",
    "        # Display interim results, but with a carriage return at the end of the\n",
    "        # line, so subsequent lines will overwrite them.\n",
    "        \n",
    "        # 暫定の結果を表示する。ただし、行末にキャリッジリターン？？？があるので\n",
    "        # 後続の行はこれらを上書きします。\n",
    "        #\n",
    "        # If the previous result was longer than this one, we need to print\n",
    "        # some extra spaces to overwrite the previous result\n",
    "        \n",
    "        # もし一個前の結果が今回のものよりも長かったら、我々は追加のスペースを\n",
    "        # 前回の結果を上書きするために追加しなければならない。\n",
    "        overwrite_chars = ' ' * (num_chars_printed - len(transcript))\n",
    "\n",
    "        if not result.is_final:\n",
    "            sys.stdout.write(transcript + overwrite_chars + '\\r')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            num_chars_printed = len(transcript)\n",
    "\n",
    "        else:\n",
    "            print_str = transcript + overwrite_chars\n",
    "            \n",
    "            if sentiment:\n",
    "                \n",
    "                mag, score = analyze_sentiment(transcript + overwrite_chars, get_native_encoding_type())['documentSentiment'].values()\n",
    "                print_str += '\\n感情の正負:{} 感情の強さ:{}\\n'.format(score, mag)\n",
    "                \n",
    "            yield print_str\n",
    "            \n",
    "\n",
    "            # Exit recognition if any of the transcribed phrases could be\n",
    "            # one of our keywords.\n",
    "            \n",
    "            # 転記されたフレーズのいずれかがGoogleのキーワードの1つになる可能性がある場合は\n",
    "            # 認識を終了してください。\n",
    "            if re.search(r'\\b(exit|quit)\\b', transcript, re.I):\n",
    "                print('Exiting..')\n",
    "                break\n",
    "\n",
    "            num_chars_printed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐍メイン関数\n",
    "- 言語コード（[詳細](https://cloud.google.com/speech-to-text/docs/languages)）と、感情推定するか否を引数にとる\n",
    "- これ呼び出すべし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lang='ja-JP', sentiment=False):\n",
    "    # See http://g.co/cloud/speech/docs/languages\n",
    "    # for a list of supported languages.\n",
    "    language_code = lang  # a BCP-47 language tag\n",
    "\n",
    "    client = speech.SpeechClient()\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=language_code)\n",
    "    streaming_config = types.StreamingRecognitionConfig(\n",
    "        config=config,\n",
    "        interim_results=True)\n",
    "\n",
    "    with MicrophoneStream(RATE, CHUNK) as stream:\n",
    "        audio_generator = stream.generator()\n",
    "        requests = (types.StreamingRecognizeRequest(audio_content=content)\n",
    "                    for content in audio_generator)\n",
    "\n",
    "        responses = client.streaming_recognize(streaming_config, requests)\n",
    "        \n",
    "        loopitr = listen_print_loop(responses, sentiment)\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # Now, put the transcription responses to use.\n",
    "                print(next(loopitr))\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\n🌸🌸🌸Interrupted!!!🌸🌸🌸')\n",
    "            return 0\n",
    "        except :\n",
    "            return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐍実行部分\n",
    "- main（言語コード, 感情推定OnOff）\n",
    "- もし65秒のタイムアウトで中断してしまった場合は、もう一度実行するように無限ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    result = main('ja-JP', True)\n",
    "    if result == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
